# 文件索引

文件索引算法允许我们能够以较快地速度查询到目标文件的存储地址，其最典型的实现即是 MySQL 等关系型数据库索引。典型的索引譬如在内存中维护一个二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在 `O(log2n)` 的复杂度内获取到相应数据。

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g10cogz8p1j30gc089dge.jpg)

左侧为数据记录的物理地址，右侧为查找树，需要注意的是，逻辑上相邻的记录在磁盘上也并不是一定物理相邻的。实际的数据库应用中我们往往使用 B+ 树或者 LSM 来替代二叉查找树或者红黑树来构建索引系统，并且充分利用[虚拟存储管理 https://url.wx-coder.cn/PeNqS](https://url.wx-coder.cn/PeNqS) 一节中介绍过的局部性原理、磁盘预读与页缓存等概念。

索引不是建立的越多、越长越好，因为索引除了占用空间之外，对后续数据库的增加、删除、修改都有额外的操作来更新索引，所以对索引列和字符串前缀长度，都参考选择性(Selectivity)这个指标来确定：选择性定义为不重复的索引值和数据总记录条数的比值，其选择性越高，那么索引的查询效率也越高，对于性别这种参数，建立索引根本没有意义。

# 二叉树

# 哈希索引

[![image.png](https://i.postimg.cc/zvwK1jyc/image.png)](https://postimg.cc/H8xVwb4w)

哈希索引即是基于哈希技术，如上图所示，我们将一系列的最终的键值通过哈希函数转化为存储实际数据桶的地址数值。值本身存储的地址就是基于哈希函数的计算结果，而搜索的过程就是利用哈希函数从元数据中推导出桶的地址。

- 添加新值的流程，首先会根据哈希函数计算出存储数据的地址，如果该地址已经被占用，则添加新桶并重新计算哈希函数。

- 更新值的流程则是先搜索到目标值的地址，然后对该内存地址应用所需的操作。

哈希索引会在进行相等性测试（等或者不等）时候具有非常高的性能，但是在进行比较查询、Order By 等更为复杂的场景下就无能为力。

# B-Tree

在[数据结构与算法/查找树 https://url.wx-coder.cn/9PnzG](https://url.wx-coder.cn/9PnzG) 一节中我们介绍了 B-Tree 的基本概念与实现，这里我们继续来分析下为何 B-Tree 相较于红黑树等二叉查找树会更适合于作为数据库索引的实现。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 IO 消耗，相对于内存存取，IO 存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘 IO 操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘 IO 的存取次数。

根据 B-Tree 的定义，可知检索一次最多需要访问 h 个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次 IO 就可以完全载入。每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个节点只需一次 IO。而检索的时候，一次检索最多需要 h-1 次 IO（根节点常驻内存），其渐进复杂度为 $O(h)=O(log_dN)O(h)=O(log_dN)$，实际应用中，出度 d 是非常大的数字，通常超过 100，因此 h 非常小（通常不超过 3）。而红黑树这种结构，h 明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的 IO 渐进复杂度也为 O(h)，效率明显比 B-Tree 差很多。

# B+Tree

B+Tree 是 B-Tree 的变种，有着比 B-Tree 更高的查询性能，其相较于 B-Tree 有了如下的变化：

- 有 m 个子树的节点包含有 m 个元素（B-Tree 中是 m-1）。

- 根节点和分支节点中不保存数据，只用于索引，所有数据都保存在叶子节点中。

- 所有分支节点和根节点都同时存在于子节点中，在子节点元素中是最大或者最小的元素。

- 叶子节点会包含所有的关键字，以及指向数据记录的指针，并且叶子节点本身是根据关键字的大小从小到大顺序链接。

一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 的基础上进行了优化，增加了顺序访问指针：

![](https://ww1.sinaimg.cn/large/007rAy9hgy1g1l0heh15xj30fu09mq3y.jpg)

如上图所示，在 B+Tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 B+Tree。做这个优化的目的是为了提高区间访问的性能，例如下图中如果要查询 key 为从 3 到 8 的所有数据记录，当找到 3 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。

![](https://i.postimg.cc/j5Z7j5xs/image.png)

# LSM-Tree

B-Tree 这种数据库索引方式是传统关系型数据库中主要的索引构建方式，然而 B-Tree 通常会存在写操作吞吐量上的瓶颈，其需要大量的磁盘随机 IO，很显然，大量的磁盘随机 IO 会严重影响索引建立的速度。对于那些索引数据大的情况(例如，两个列的联合索引)，插入速度是对性能影响的重要指标，而读取相对来说就比较少。譬如在一个无缓存的情况下，B-Tree 首先需要进行一次磁盘读写将磁盘页读取到内存中，然后进行修改，最后再进行一次 IO 写回到磁盘中。

LSM-Tree 则采取读写分离的策略，会优先保证写操作的性能；其数据首先存储内存中，而后需要定期 Flush 到硬盘上。LSM-Tree 通过内存插入与磁盘的顺序写，来达到最优的写性能，因为这会大大降低磁盘的寻道次数，一次磁盘 IO 可以写入多个索引块。HBase, Cassandra, RockDB, LevelDB, SQLite 等都是基于 LSM-Tree 来构建索引的数据库；LSM-Tree 的树节点可以分为两种，保存在内存中的称之为 MemTable, 保存在磁盘上的称之为 SSTable。

![image](https://i.postimg.cc/CMn80FyV/51817950-f691bc00-2307-11e9-9492-b819d7a61ec0.png)

LSM-tree 的主要思想是划分不同等级的树。以两级树为例，可以想象一份索引数据由两个树组成，一棵树存在于内存，一棵树存在于磁盘。内存中的树可以可以是 AVL Tree 等结构；因为数据大小是不同的，没必要牺牲 CPU 来达到最小的树高度。而存在于磁盘的树是一棵 B-Tree。

![](http://dl.iteye.com/upload/picture/pic/118173/9092b78b-5c7a-37df-b9f2-fb8038bb79b9.jpg)

数据首先会插入到内存中的树。当内存中的树中的数据超过一定阈值时，会进行合并操作。合并操作会从左至右遍历内存中的树的叶子节点与磁盘中的树的叶子节点进行合并，当被合并的数据量达到磁盘的存储页的大小时，会将合并后的数据持久化到磁盘，同时更新父亲节点对叶子节点的指针。

![](http://dl.iteye.com/upload/picture/pic/118175/7ece3749-415a-3083-893e-6859c9b9fc78.jpg)

之前存在于磁盘的叶子节点被合并后，旧的数据并不会被删除，这些数据会拷贝一份和内存中的数据一起顺序写到磁盘。这会操作一些空间的浪费，但是，LSM-Tree 提供了一些机制来回收这些空间。磁盘中的树的非叶子节点数据也被缓存在内存中。数据查找会首先查找内存中树，如果没有查到结果，会转而查找磁盘中的树。有一个很显然的问题是，如果数据量过于庞大，磁盘中的树相应地也会很大，导致的后果是合并的速度会变慢。一个解决方法是建立各个层次的树，低层次的树都比 上一层次的树数据集大。假设内存中的树为 c0, 磁盘中的树按照层次一次为 `c1, c2, c3, ... ck-1, ck`，合并的顺序是 `(c0, c1), (c1, c2)...(ck-1, ck)`。
